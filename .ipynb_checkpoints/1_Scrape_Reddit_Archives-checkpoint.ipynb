{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/swlh/how-to-scrape-large-amounts-of-reddit-data-using-pushshift-1d33bde9286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1625331971\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import platform\n",
    "import time\n",
    "\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "        \n",
    "api = PushshiftAPI()\n",
    "\n",
    "import datetime as dt\n",
    "before = int(dt.datetime(2021,6,26,0,0).timestamp())\n",
    "after = int(dt.datetime(2020,12,1,0,0).timestamp())\n",
    "\n",
    "print(int(time.time()))\n",
    "\n",
    "subreddits = [\"wallstreetbets\", \"stocks\", \"wallstreetbetsOGs\", \"spacs\", \"investing\", \"pennystocks\", \"stockmarket\", \"options\", \"robinhoodpennystocks\", \"wallstreetbetsnew\", \"smallstreetbets\"]\n",
    "subreddits = [\"stocks\", \"wallstreetbetsOGs\", \"spacs\", \"investing\", \"pennystocks\", \"stockmarket\", \"options\", \"robinhoodpennystocks\", \"wallstreetbetsnew\", \"smallstreetbets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Update a target subreddit with new data\n",
    "#//*** subreddit = subbreddit name\n",
    "#//*** Method = 'before' or 'after' indicating if the records to be retrieved are before or after the target_utc. Defaults to After\n",
    "#//*** limit is the number or records to grab\n",
    "def update_subreddit(subreddit,method,limit):\n",
    "    import time\n",
    "    filename = f\".\\\\data\\\\{subreddit}_comments.csv.zip\"\n",
    "\n",
    "    #//*** Convert Path to Mac formatting if needed\n",
    "    if platform.system() == 'Darwin':\n",
    "        filename = filename.replace(\"\\\\\",\"/\")\n",
    "    \n",
    "    #//*** Check if File exists\n",
    "    if os.path.isfile(filename):\n",
    "        print (\"Update Existing File\")\n",
    "\n",
    "        print(f\"Reading csv: {filename}\")\n",
    "        start_time = time.time()\n",
    "        update_df = pd.read_csv(filename, compression = 'zip')\n",
    "\n",
    "        print(f\"csv loaded: {round(time.time()-start_time,2)}s\")\n",
    "\n",
    "        print(f\"csv Record count: {len(update_df)}\")\n",
    "\n",
    "        #//*** If before, get the largest (latest) utc\n",
    "        if method == 'before':\n",
    "            #//*** Get the Before utc from the stored csv\n",
    "            before = update_df['created_utc'].min() \n",
    "\n",
    "            print(f\"Getting {limit} records before {before} utc\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            #//*** Download comments\n",
    "            comments = api.search_comments(subreddit=subreddit, limit=limit, before=before)\n",
    "\n",
    "            print(f\"Download Time: {round(time.time()-start_time,2)}s\")\n",
    "\n",
    "        elif method == 'after':\n",
    "            after = update_df['created_utc'].max() \n",
    "\n",
    "            print(f\"Getting {limit} records after {after} utc\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            #//*** Download comments\n",
    "            comments = api.search_comments(subreddit=subreddit, limit=limit, after=after)\n",
    "            print(f\"Download Time: {round(time.time()-start_time,2)}s\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Method needs to be 'before' or 'after': [{method}] is invalid\")\n",
    "            return\n",
    "    else:\n",
    "        #//*** This is a new file, download 100k records starting right now\n",
    "        after = int(time.time())\n",
    "        before = int(time.time())\n",
    "        \n",
    "        limit = 1000000\n",
    "        limit = 500000\n",
    "        \n",
    "        print(f\"Getting {limit} records before {after} utc\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        update_df = pd.DataFrame()\n",
    "\n",
    "        #//*** Download comments\n",
    "        comments = api.search_comments(subreddit=subreddit, limit=limit, before=before)\n",
    "\n",
    "        print(f\"Download Time: {round(time.time()-start_time,2)}s\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    #//***************************************************************************\n",
    "    #//*** Download Complete\n",
    "    #//***************************************************************************\n",
    "    \n",
    "    #//*** Convert comments to Dataframe\n",
    "    raw_df = pd.DataFrame(comments)\n",
    "\n",
    "    if len(raw_df) == 0:\n",
    "        \n",
    "        print(f\"{subreddit} {method} {limit} - No Records Returned\")\n",
    "        return\n",
    "    \n",
    "    #//*** Columns to keep\n",
    "    keep_cols = [\"score\",\"total_awards_received\",\"created_utc\",\"is_submitter\",\"author_fullname\",\"body\",\"id\",\"link_id\",\"parent_id\",\"stickied\",\"permalink\",\"retrieved_on\",\"subreddit\",\"subreddit_id\"]\n",
    "    \n",
    "    #//*** Not all columns appear. This usually happens with small samples used for testing.\n",
    "    #//*** Only use the keep_cols that are actually in the sample. The missing columns will be added during concat later\n",
    "    actual_cols = []\n",
    "    \n",
    "    #//*** Loop through each column we want to keep\n",
    "    for col in keep_cols:\n",
    "        #//*** Add col to actual_cols if it exists\n",
    "        if col in raw_df.columns:\n",
    "            actual_cols.append(col)\n",
    "\n",
    "    #//*** Keep the important columns\n",
    "    raw_df = raw_df[actual_cols]\n",
    "    \n",
    "    print(f\"Checking For Duplicates - Length Before: {len(raw_df)}\")\n",
    "    \n",
    "    #//*** Hash the body, will use to check for duplicates\n",
    "    #//*** Hash the body using sha-256\n",
    "    #Sha256: Reference https://www.pythonpool.com/python-sha256/\n",
    "\n",
    "    raw_df['hash'] = raw_df['body'].apply(lambda x:hashlib.sha256(x.encode()).hexdigest())\n",
    "\n",
    "\n",
    "    # dropping Duplicates First. No sense in processing these\n",
    "    raw_df.drop_duplicates(subset =\"hash\",keep = False, inplace = True)\n",
    "    \n",
    "    print(f\"Checking For Duplicates - Length After: {len(raw_df)}\")\n",
    "\n",
    "    #print(\"Begin Cleaning\")\n",
    "\n",
    "    #//*** Clean text, tokenize and remove stop words\n",
    "    #raw_df['clean'] = remove_stop_words(tokenize_series(mr_clean_text(raw_df['body'],{\"remove_empty\":False})))\n",
    "    \n",
    "    #//*** encode the comments\n",
    "    #//*** Breaking this out into a separate function for readability and possible future flexibility\n",
    "    #raw_df = encode_comments(raw_df)\n",
    "    \n",
    "    #//*** Combining existing dataframe with raw_df\n",
    "    update_df = pd.concat([update_df,raw_df])\n",
    "    print(f\"Combined Dataframe Size:{len(update_df)}\")\n",
    "\n",
    "    # Check for Duplicates. \n",
    "    update_df.drop_duplicates(subset =\"hash\",keep = False, inplace = True)\n",
    "    print(f\"Dropping Duplicates - New Size:{len(update_df)}\")\n",
    "\n",
    "    #print(\"Replace NaN with Zeros\")\n",
    "    update_df = update_df.fillna(0)\n",
    "    \n",
    "    #//*** Sort the Dataframe by UTC date. This keeps the time series chronological. \n",
    "    #//*** No need to reindex, since index will be ignored at csv read/write\n",
    "    update_df = update_df.sort_values('created_utc')\n",
    "\n",
    "    #//*** Convert is Submitter,Stickied field to Boolean.\n",
    "    #//*** Some early values are Integers and Strings\n",
    "    update_df['is_submitter'] = update_df['is_submitter'].astype('bool')\n",
    "    update_df['stickied'] = update_df['stickied'].astype('bool')\n",
    "    update_df['author_fullname'] = update_df['author_fullname'].astype('str')\n",
    "    \n",
    "    print(f\"Writing {filename}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    update_df.to_csv(filename,compression=\"zip\",index=False)    \n",
    "    \n",
    "    print(f\"File Written: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    print(f\"update_subreddit() Complete: {len(update_df)} records\")\n",
    "    \n",
    "    del update_df\n",
    "    del raw_df\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating: stocks\n",
      "Update Existing File\n",
      "Reading csv: .\\data\\stocks_comments.csv.zip\n",
      "csv loaded: 3.77s\n",
      "csv Record count: 440629\n",
      "Getting 1000 records before 1593741188 utc\n",
      "Total:: Success Rate: 100.00% - Requests: 10 - Batches: 1 - Items Remaining: 0\n",
      "Download Time: 11.66s\n",
      "Checking For Duplicates - Length Before: 1000\n",
      "Checking For Duplicates - Length After: 966\n",
      "Combined Dataframe Size:441595\n",
      "Dropping Duplicates - New Size:441563\n",
      "Writing .\\data\\stocks_comments.csv.zip\n",
      "File Written: 13.1s\n",
      "update_subreddit() Complete: 441563 records\n",
      "Updating: wallstreetbetsOGs\n",
      "Update Existing File\n",
      "Reading csv: .\\data\\wallstreetbetsOGs_comments.csv.zip\n",
      "csv loaded: 3.47s\n",
      "csv Record count: 451021\n",
      "Getting 1000 records before 1611978410 utc\n",
      "1000 result(s) not found in Pushshift\n",
      "Download Time: 0.44s\n",
      "wallstreetbetsOGs before 1000 - No Records Returned\n",
      "Updating: spacs\n",
      "Update Existing File\n",
      "Reading csv: .\\data\\spacs_comments.csv.zip\n",
      "csv loaded: 3.87s\n",
      "csv Record count: 448156\n",
      "Getting 1000 records before 1593746934 utc\n",
      "Total:: Success Rate: 54.29% - Requests: 35 - Batches: 6 - Items Remaining: 0\n",
      "Download Time: 14.88s\n",
      "Checking For Duplicates - Length Before: 1000\n",
      "Checking For Duplicates - Length After: 970\n",
      "Combined Dataframe Size:449126\n",
      "Dropping Duplicates - New Size:449120\n",
      "Writing .\\data\\spacs_comments.csv.zip\n",
      "File Written: 11.47s\n",
      "update_subreddit() Complete: 449120 records\n",
      "Updating: investing\n",
      "Update Existing File\n",
      "Reading csv: .\\data\\investing_comments.csv.zip\n",
      "csv loaded: 3.9s\n",
      "csv Record count: 420718\n",
      "Getting 1000 records before 1593758138 utc\n",
      "Total:: Success Rate: 100.00% - Requests: 10 - Batches: 1 - Items Remaining: 0\n",
      "Download Time: 20.87s\n",
      "Checking For Duplicates - Length Before: 1000\n",
      "Checking For Duplicates - Length After: 937\n",
      "Combined Dataframe Size:421655\n",
      "Dropping Duplicates - New Size:421647\n",
      "Writing .\\data\\investing_comments.csv.zip\n",
      "File Written: 13.6s\n",
      "update_subreddit() Complete: 421647 records\n",
      "Updating: pennystocks\n",
      "Getting 500000 records before 1625332089 utc\n",
      "Total:: Success Rate: 73.06% - Requests: 7238 - Batches: 724 - Items Remaining: 0\n",
      "Download Time: 7353.26s\n",
      "Checking For Duplicates - Length Before: 500000\n",
      "Checking For Duplicates - Length After: 412154\n",
      "Combined Dataframe Size:412154\n",
      "Dropping Duplicates - New Size:412154\n",
      "Writing .\\data\\pennystocks_comments.csv.zip\n",
      "File Written: 10.5s\n",
      "update_subreddit() Complete: 412154 records\n",
      "Updating: stockmarket\n",
      "Getting 500000 records before 1625339461 utc\n",
      "Total:: Success Rate: 82.32% - Requests: 4565 - Batches: 457 - Items Remaining: 180355\n",
      "Total:: Success Rate: 86.58% - Requests: 6513 - Batches: 652 - Items Remaining: 22112\n",
      "Total:: Success Rate: 87.02% - Requests: 6735 - Batches: 675 - Items Remaining: 0\n",
      "Download Time: 6837.35s\n",
      "Checking For Duplicates - Length Before: 500000\n",
      "Checking For Duplicates - Length After: 456443\n",
      "Combined Dataframe Size:456443\n",
      "Dropping Duplicates - New Size:456443\n",
      "Writing .\\data\\stockmarket_comments.csv.zip\n",
      "File Written: 14.83s\n",
      "update_subreddit() Complete: 456443 records\n",
      "Updating: options\n",
      "Getting 500000 records before 1625346324 utc\n",
      "Total:: Success Rate: 75.27% - Requests: 6394 - Batches: 641 - Items Remaining: 93163\n",
      "Total:: Success Rate: 74.80% - Requests: 7742 - Batches: 776 - Items Remaining: 0\n",
      "Download Time: 7880.26s\n",
      "Checking For Duplicates - Length Before: 500000\n",
      "Checking For Duplicates - Length After: 451565\n",
      "Combined Dataframe Size:451565\n",
      "Dropping Duplicates - New Size:451565\n",
      "Writing .\\data\\options_comments.csv.zip\n",
      "File Written: 15.33s\n",
      "update_subreddit() Complete: 451565 records\n",
      "Updating: robinhoodpennystocks\n",
      "Getting 500000 records before 1625354229 utc\n",
      "Total:: Success Rate: 85.86% - Requests: 3720 - Batches: 372 - Items Remaining: 230931\n",
      "Total:: Success Rate: 80.04% - Requests: 7299 - Batches: 731 - Items Remaining: 7459\n",
      "Total:: Success Rate: 80.18% - Requests: 7380 - Batches: 740 - Items Remaining: 0\n",
      "Download Time: 7489.57s\n",
      "Checking For Duplicates - Length Before: 500000\n",
      "Checking For Duplicates - Length After: 428037\n",
      "Combined Dataframe Size:428037\n",
      "Dropping Duplicates - New Size:428037\n",
      "Writing .\\data\\robinhoodpennystocks_comments.csv.zip\n",
      "File Written: 12.08s\n",
      "update_subreddit() Complete: 428037 records\n",
      "Updating: wallstreetbetsnew\n",
      "Getting 500000 records before 1625361741 utc\n",
      "Total:: Success Rate: 69.71% - Requests: 8107 - Batches: 811 - Items Remaining: 0\n",
      "Download Time: 8210.03s\n",
      "Checking For Duplicates - Length Before: 500000\n",
      "Checking For Duplicates - Length After: 381419\n",
      "Combined Dataframe Size:381419\n",
      "Dropping Duplicates - New Size:381419\n",
      "Writing .\\data\\wallstreetbetsnew_comments.csv.zip\n",
      "File Written: 8.61s\n",
      "update_subreddit() Complete: 381419 records\n",
      "Updating: smallstreetbets\n",
      "Getting 500000 records before 1625369968 utc\n",
      "344083 result(s) not found in Pushshift\n",
      "Total:: Success Rate: 74.26% - Requests: 1896 - Batches: 191 - Items Remaining: 36881\n",
      "74 result(s) not found in Pushshift\n",
      "Total:: Success Rate: 74.06% - Requests: 2498 - Batches: 254 - Items Remaining: 0\n",
      "Download Time: 2506.49s\n",
      "Checking For Duplicates - Length Before: 155843\n",
      "Checking For Duplicates - Length After: 138856\n",
      "Combined Dataframe Size:138856\n",
      "Dropping Duplicates - New Size:138856\n",
      "Writing .\\data\\smallstreetbets_comments.csv.zip\n",
      "File Written: 3.45s\n",
      "update_subreddit() Complete: 138856 records\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#//*** Add 100k Comments to wallstreetbets\n",
    "#update_subreddit(\"wallstreetbets\",\"after\",100)\n",
    "\n",
    "#update_subreddit(\"stocks\",\"after\",100)\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    print(f\"Updating: {subreddit}\")\n",
    "    update_subreddit(subreddit,\"before\",1000)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(raw_df['hash'].unique()))\n",
    "#print(len(raw_df.tail()))\n",
    "#raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Reference to manually read & write to Data Frame\n",
    "#filename = f\".\\\\data\\\\wallstreetbets_comments.csv.zip\"\n",
    "#update_df = pd.read_csv(filename, compression = 'zip')\n",
    "#print(len(update_df))\n",
    "#update_df\n",
    "#filename = f\".\\\\data\\\\wallstreetbets_comments_comments.csv\"\n",
    "#update_df.to_csv(filename, compression = 'zip',index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_df.to_csv(\"reddit_comments.csv.zip\",compression=\"zip\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Push shift scraper reference\n",
    "\"\"\"\n",
    "#//*** Download the First 100,000 Comments from reddit pushsift\n",
    "subreddit=\"wallstreetbets\"\n",
    "limit=100\n",
    "#comments = api.search_comments(subreddit=subreddit, limit=limit, before=before after=after)\n",
    "comments = api.search_comments(subreddit=subreddit, limit=limit, after=after)\n",
    "print(f'Retrieved {len(comments)} comments from Pushshift')\n",
    "\n",
    "#//*** Convert comments to Dataframe\n",
    "comments_df = pd.DataFrame(comments)\n",
    "\n",
    "#//*** Save DataFrame to a file for processing\n",
    "comments_df.to_csv(f\"{subreddit}_raw_comments.csv.zip\",compression=\"zip\",index=False)\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Reference to create Stock ticker count matrix\n",
    "\"\"\"\n",
    "\n",
    "#//*** Build list of ticker symbols from NYSE and NASDAQ\n",
    "#//*** Reads from Excel file.\n",
    "#//*** Gets the Symbol column, and converts to lower case, \n",
    "nyse = pd.read_csv(\"NYSE_20210625.csv\",header=None)[0].str.lower()\n",
    "nasdaq = pd.read_csv(\"NASDAQ_20210625.csv\",header=None)[0].str.lower()\n",
    "\n",
    "#//*** Removes symbols with 1 and 2 character listings\n",
    "nyse = list(nyse[nyse.apply(lambda x: len(x)>2) ])\n",
    "nasdaq = list(nasdaq[nasdaq.apply(lambda x: len(x)>2) ])\n",
    "\n",
    "symbols = nyse + nasdaq\n",
    "\n",
    "#//*** Count each Stock mention add it to a dictionary of lists. Each list is filled with 0s. The Specific row index is updated with the relevant count. \n",
    "#//*** This Generates a word count matrix\n",
    "stock_dict = {}\n",
    "\n",
    "#//*** Keep Track of Rows\n",
    "index = 0\n",
    "\n",
    "for row in raw_df.iterrows():\n",
    "    \n",
    "    #//*** Get the cleaned body text\n",
    "    body = row[1]['clean']\n",
    "    \n",
    "    #//*** For Each Stock Symbol\n",
    "    for stock in symbols:\n",
    "        \n",
    "        #//*** Check if Stock exists in Body\n",
    "        if stock in body:\n",
    "            \n",
    "            #//*** Reset the stock counter\n",
    "            count = 0\n",
    "            \n",
    "            #//*** Loop through body and county ticker mentions\n",
    "            for word in body:\n",
    "                #//*** If word found increment count\n",
    "                if stock == word:\n",
    "                    count += 1\n",
    "                    \n",
    "            #//*** Check if symbol is in stock_dict\n",
    "            if stock not in stock_dict.keys():    \n",
    "\n",
    "                #//*** If not, then build it\n",
    "                stock_dict[stock] = np.zeros(len(raw_df))\n",
    "            \n",
    "            #//*** Update the stock value at the \n",
    "            stock_dict[stock][index] = count\n",
    "\n",
    "    #//*** Increment Index to keep with row index\n",
    "    index +=1   \n",
    "\n",
    "#//*** Loop through the dictionary key and lists\n",
    "for col,values in stock_dict.items():\n",
    "    \n",
    "    #//*** Add each key (which is a stock ticker symbol) as a column using the list of ticker counts for Data\n",
    "    raw_df[col] = values.astype('int')\n",
    "    \n",
    "\"\"\"\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
